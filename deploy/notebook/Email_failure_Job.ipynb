{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f023c-f908-4c68-b56b-71ddaf339a71",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "ad0ce492-dab2-49f7-a2a9-86a8cc28976b",
       "queued_time": "2024-12-24T14:19:47.2646087Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intial_load_df = spark.read.parquet(parquet_file_path, header= True)\n",
    "\n",
    "#Create intial table\n",
    "#intial_load_df.createOrReplaceGlobalTempView(\"intial_load\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE error_logs(\n",
    "    notebook_name VARCHAR(255),\n",
    "    error_logs VARCHAR(255),\n",
    "    CURRENT_TIMESTAMP VARCHAR(255)\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71557544-42df-4b5a-8a4b-368caa424001",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "116bce8e-afdd-41c9-b730-093596b962fd",
       "queued_time": "2024-12-24T15:52:14.5758618Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE error_logs\n",
    "    ADD COLUMNS (run_time VARCHAR(255))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9e545-a679-42cc-9c9f-043f6768987f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ded54-1b95-4773-9fc4-fb5e805a9d5a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "930b85f9-1c0c-45ed-bb66-605b49b410c5",
       "queued_time": "2024-12-26T11:24:26.727089Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Perform the multiplication\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     c \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m \u001b[43mw\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Log the result\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m error_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Log the error\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241m.\u001b[39merror(error_log)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Raise the exception again (optional if you want to propagate the error)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_log)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# a = 5\n",
    "# b = 9\n",
    "\n",
    "# c = a * b\n",
    "\n",
    "# print(f'Hey, there, the value of c is {c}')\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "# Variables\n",
    "a = 5\n",
    "b = 9\n",
    "d = 343678\n",
    "# Define Eastern Time Zone\n",
    "eastern = pytz. timezone ('US/Eastern')\n",
    "datestr = datetime.now(eastern).strftime(\"%Y%m%d%H&M%S\")\n",
    "\n",
    "\n",
    "#Define log path\n",
    "log_path = '/lakehouse/default/Files/Logs_Generic/'\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs (log_path, exist_ok=True)\n",
    "\n",
    "file_name = 'Email_failure_Job_ '+ datestr\n",
    "\n",
    "# Configure logging \n",
    "logging. basicConfig(\n",
    "filename=os.path. join(log_path, file_name + ' .log'), \n",
    "force=True, \n",
    "filemode= 'w',\n",
    "level=logging. INFO)\n",
    "\n",
    "try:\n",
    "    # Perform the multiplication\n",
    "    c = a * b\n",
    "    \n",
    "    \n",
    "    # Log the result\n",
    "    logger.info(f'Hey, there, the value of c is {c}')\n",
    "    \n",
    "except Exception as e:\n",
    "    # If an exception occurs, store the error message in the error_log variable\n",
    "    error_log = f\"Error occurred: {str(e)}\"\n",
    "    \n",
    "    # Log the error\n",
    "    logger.error(error_log)\n",
    "    \n",
    "    # Raise the exception again (optional if you want to propagate the error)\n",
    "    raise Exception(error_log)\n",
    "\n",
    "# If error_log is not None, log the error log (additional check to ensure it was populated)\n",
    "if error_log:\n",
    "    logger.info(f\"Error Log: {error_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b04fa-0700-4b1f-8762-07680a5d77ea",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "7fbfa46c-c129-4494-9aaf-a5fa6506bf54",
       "queued_time": "2024-12-26T11:39:16.6564284Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'prints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m error_message \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[1;32m     29\u001b[0m log_error_to_delta(error_message, runtime)  \u001b[38;5;66;03m# Log the error and runtime\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e  \u001b[38;5;66;03m# Re-raise the exception to stop the notebook\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Simulate a task that will raise an error (you can replace this with your actual code)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mprints\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabc\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Typo in 'prints', should be 'print'\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Measure elapsed time when exception occurs\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prints' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ErrorLoggingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to log errors\n",
    "def log_error_to_delta(error_message, notebook_runtime, table_name=\"errorlogging_test\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    notebook = \"Email_failure_Job\"\n",
    "    error_data = [(timestamp, notebook, error_message, notebook_runtime)]\n",
    "    error_df = spark.createDataFrame(error_data, [\"timestamp\", \"notebook\", \"error_message\", \"execution_time\"])\n",
    "    error_df.write.format(\"delta\").option(\"mergeSchema\", \"True\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Simulate a task that will raise an error (you can replace this with your actual code)\n",
    "    print(\"abc\")  # Typo in 'prints', should be 'print'\n",
    "except Exception as e:\n",
    "    # Measure elapsed time when exception occurs\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "    error_message = traceback.format_exc()\n",
    "    log_error_to_delta(error_message, runtime)  # Log the error and runtime\n",
    "    raise e  # Re-raise the exception to stop the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82392db6-d96f-42f6-919b-58ded164e219",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "927d70ad-6ac7-4d90-bbbd-95b365d47d1c",
       "queued_time": "2024-12-24T16:10:37.3680057Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'error_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(\u001b[43merror_log\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error_log' is not defined"
     ]
    }
   ],
   "source": [
    "display(error_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f594c3d-a0ce-4d4d-8ac6-0bcef9335269",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "ec86bed1-8693-478e-8817-ef77d2d6dd3f",
       "queued_time": "2024-12-24T15:54:48.7537346Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to infer the type of the field current_timestamp.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1529\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1566\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1568\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'set'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1574\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1571\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1572\u001b[0m         StructField(\n\u001b[1;32m   1573\u001b[0m             k,\n\u001b[0;32m-> 1574\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1580\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1581\u001b[0m         )\n\u001b[1;32m   1582\u001b[0m     )\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1535\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'set'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1529\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1584\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer the type of the field \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to infer the type of the field _field_names.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1574\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1571\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1572\u001b[0m         StructField(\n\u001b[1;32m   1573\u001b[0m             k,\n\u001b[0;32m-> 1574\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1580\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1581\u001b[0m         )\n\u001b[1;32m   1582\u001b[0m     )\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1535\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'py4j.java_gateway.JavaObject'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1529\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1584\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer the type of the field \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to infer the type of the field _jc.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1574\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1571\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1572\u001b[0m         StructField(\n\u001b[1;32m   1573\u001b[0m             k,\n\u001b[0;32m-> 1574\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1580\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1581\u001b[0m         )\n\u001b[1;32m   1582\u001b[0m     )\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1535\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'pyspark.sql.column.Column'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m data_types \u001b[38;5;241m=\u001b[39m [StringType(), StringType(), TimestampType(), StringType()]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Create the DataFrame with specified schema and types\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m error_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Show the DataFrame for verification\u001b[39;00m\n\u001b[1;32m     30\u001b[0m error_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1275\u001b[0m     )\n\u001b[0;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1318\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:962\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    959\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 962\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    964\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:836\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    834\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    835\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 836\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:839\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    834\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    835\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    836\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    837\u001b[0m     _merge_type,\n\u001b[1;32m    838\u001b[0m     (\n\u001b[0;32m--> 839\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m    847\u001b[0m     ),\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:1584\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1572\u001b[0m             StructField(\n\u001b[1;32m   1573\u001b[0m                 k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1581\u001b[0m             )\n\u001b[1;32m   1582\u001b[0m         )\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer the type of the field \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to infer the type of the field current_timestamp."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"NotebookMetadataLogging\").getOrCreate()\n",
    "\n",
    "# Manually set the notebook name (this needs to be updated as per your notebook context)\n",
    "notebook_name = \"notebook_01\"  # Manually set the notebook name here\n",
    "error_logs = \"An example error description\"  # Example error message\n",
    "timestamp = current_timestamp()  # Current timestamp of the execution\n",
    "\n",
    "# Optionally, get environment variables related to the notebook execution (if available)\n",
    "# Example: You could retrieve additional metadata if it's stored in environment variables\n",
    "run_time = os.environ.get(\"FABRIC_RUN_TIME\", \"unknown\")  # Replace with actual environment variable for runtime\n",
    "\n",
    "# Create the DataFrame with the correct types\n",
    "error_data = [(notebook_name, error_logs, timestamp, run_time)]\n",
    "columns = [\"notebook_name\", \"error_logs\", \"current_timestamp\", \"run_time\"]\n",
    "\n",
    "# Define schema explicitly to avoid type inference issues\n",
    "schema = [\"notebook_name\", \"error_logs\", \"current_timestamp\", \"run_time\"]\n",
    "data_types = [StringType(), StringType(), TimestampType(), StringType()]\n",
    "\n",
    "# Create the DataFrame with specified schema and types\n",
    "error_df = spark.createDataFrame(error_data, schema=schema)\n",
    "\n",
    "# Show the DataFrame for verification\n",
    "error_df.show()\n",
    "\n",
    "# Insert the data into the Delta table (assuming the table exists)\n",
    "error_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"error_logs\")  # Replace with the actual table name in your Lakehouse\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291af966-ca04-4bab-8bfb-bf7bdd88057b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "4d20010c-5bb3-4705-8fa3-f39c3bf5dab9",
       "queued_time": "2024-12-26T11:39:41.472546Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "2ff2d0d1-a3aa-4109-9ba3-2df7115f22d4",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 2ff2d0d1-a3aa-4109-9ba3-2df7115f22d4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"select * from errorlogging_test\"\"\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef98784-4cf1-482d-ab2f-662a27fb1a2f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "waiting",
       "parent_msg_id": "de38a0be-755d-4748-ba11-7845dda8d74d",
       "queued_time": "2024-12-24T17:11:34.8631133Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, , Waiting)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: M00: command not found\r\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "2fec6682-017a-4d17-b372-282cc705aab3",
    "default_lakehouse_name": "Email_Failure",
    "default_lakehouse_workspace_id": "654d6099-5fef-4a37-90b2-c07d16f3f88a"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "2ff2d0d1-a3aa-4109-9ba3-2df7115f22d4": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "binsNumber": 10,
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "line",
        "evaluatesOverAllRecords": false,
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "2024-12-26 11:39:17",
         "1": "Email_failure_Job",
         "2": "Traceback (most recent call last):\n  File \"/tmp/ipykernel_5772/1493594902.py\", line 23, in <module>\n    prints(\"abc\")  # Typo in 'prints', should be 'print'\nNameError: name 'prints' is not defined\n",
         "3": "7.200241088867188E-5"
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "timestamp",
         "type": "string"
        },
        {
         "key": "1",
         "name": "notebook",
         "type": "string"
        },
        {
         "key": "2",
         "name": "error_message",
         "type": "string"
        },
        {
         "key": "3",
         "name": "execution_time",
         "type": "double"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
